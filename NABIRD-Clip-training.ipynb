{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import albumentations as A\n",
    "from torchvision import transforms, datasets\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:45<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "path_to_image = {}\n",
    "for i in tqdm(range(0, 23)):\n",
    "    path = '/scratch/sm9669/nabird pickle/nabird/train_'+str(i*1000)+'.pkl'\n",
    "    file = open(path,'rb')\n",
    "    sub_data = pickle.load(file)\n",
    "    path_to_image.update(sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/sm9669/classes.txt') as f:\n",
    "    lines = f.readlines()\n",
    "classid_name = []\n",
    "for x in lines:\n",
    "    classid_name.append( x[len(x.split(\" \")[0])+1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgpath_name(path):\n",
    "    class_id = int(path.split('/')[0])\n",
    "    return classid_name[class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = '/scratch/sm9669/nabirds_with_loc_2019.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "#     image_path = image_path\n",
    "#     captions_path = captions_path\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def default_loader(self, path):\n",
    "        image= path_to_image[path]\n",
    "        img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        im_pil = Image.fromarray(img)\n",
    "    #     return Image.open(path).convert('RGB')\n",
    "        return im_pil\n",
    "    \n",
    "    def __init__(self, meta_data, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "        meta_type = 'ebird_meta'\n",
    "        \n",
    "        print('Loading annotations from: ' + os.path.basename(meta_data))\n",
    "        with open(meta_data) as fp:\n",
    "            da = json.load(fp)\n",
    "\n",
    "        # only choose images that are available - ie some missing from download\n",
    "        # also select the split of interest\n",
    "        da = [dd for dd in da['train'] if dd['valid_image']]\n",
    "\n",
    "        # set up the filenames and annotations\n",
    "        self.imgs = [dd['im_path'] for dd in da]\n",
    "        self.classes = [dd['class_id'] for dd in da]\n",
    "        self.users = [dd[meta_type]['user_id'] for dd in da]\n",
    "        self.dates = [dd[meta_type]['date'] for dd in da]\n",
    "        self.lon = [dd[meta_type]['lon'] for dd in da]\n",
    "        self.lat = [dd[meta_type]['lat'] for dd in da]\n",
    "        self.name = [imgpath_name(dd['im_path']) for dd in da]\n",
    "        \n",
    "        \n",
    "        self.captions = [\"This is a photograph of \"+ x for x in self.name]\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(self.captions), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = path_to_image[self.imgs[idx]]\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations from: nabirds_with_loc_2019.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "dataset= CLIPDataset(meta_data, tokenizer,get_transforms())\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader= train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.image_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(model, valid_loader)\n",
    "        \n",
    "        if valid_loss.avg < best_loss:\n",
    "            best_loss = valid_loss.avg\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            print(\"Saved Best Model!\")\n",
    "        \n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 714/714 [02:25<00:00,  4.90it/s, lr=0.0001, train_loss=3.33]\n",
      "100%|██████████| 714/714 [01:14<00:00,  9.57it/s, valid_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model!\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 714/714 [02:24<00:00,  4.96it/s, lr=0.0001, train_loss=1.32]\n",
      " 93%|█████████▎| 663/714 [01:08<00:05,  9.90it/s, valid_loss=0.735]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
